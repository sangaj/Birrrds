# load the package 
import findspark
findspark.init('/data/spark-1.6.0-bin-hadoop2.6')
from pyspark import SparkContext, HiveContext
from pyspark.sql import functions as F
from pyspark.sql import Window as w
%matplotlib inline
import seaborn as sns
import pandas as pd

# load Spark and HiveContext
sc = SparkContext()
hc = HiveContext(sc)

track_table = "birds.track"
track = hc.read.table(track_table) 
# input track table which contanis trajectory
track = track.select('id','classification_id','species_id','distance_travelled','score')
# only select part of features

trackestimate_table = "birds.trackestimate"
trackestimate = hc.read.table(trackestimate_table)  
# input trackestimate table which contanis each location

track_subset = (trackestimate.where("dt > '2015-07-01 00:00:00'")
                     .where("dt < '2015-07-01 23:59:59'")).persist()
                     #.where("classification_id != 10")
                     #.where("classification_id != 5")
                     #.where("classification_id != 1"))
                     #.where("distance_travelled < 10000")) 
# select one day data based on the reqiurements
                     
start_time = '2015-07-01 00:00:01'
end_time = '2015-07-02 00:00:00'
all_times = pd.date_range(start_time,end_time, freq='s', closed='left')
# generate one day value in each seconds
all_times = hc.createDataFrame(pd.DataFrame((all_times).astype(str), columns=['ts']))
# generate a dataframe
all_times.head(10) # dataframe of each seconds
all_times.count()

weather_table = "birds.weather"
weather = hc.read.table(weather_table)
weather.persist() # read  weather data
weather.head(10)

weather_sec = weather.withColumn('ts', F.date_format('dt', 'yyyy-MM-dd HH:mm:ss'))['dt','ts']
weather_sec.show(10)
# add a new column with new time format and select the two features

ts = None
for delta_sec in range(31): 
    if ts is None:
        ts = hc.createDataFrame(pd.DataFrame((all_times+(24*60*60)*delta_sec).astype(str), columns=['ts']))
    else:
        ts = ts.unionAll(hc.createDataFrame(pd.DataFrame((all_times+(24*60*60)*delta_sec).astype(str), columns=['ts'])))
ts.head(100) # generate the time in one month

#weather_full = ts.join(weather_sec, on=weather_sec.ts==ts.ts, how='left').drop(weather_sec.ts)
weather_full = all_times.join(weather_sec, on=weather_sec.ts==all_times.ts, how='left').drop(weather_sec.ts)
weather_full = weather_full.orderBy('ts').dropDuplicates(['ts'])# join two tables one is with all seconds, one is weather data
weather_full.count()

window = w.partitionBy().orderBy('ts')   
lag_windows = [F.lag(F.col('dt'),count=lag).over(window) for lag in range(1,10)] 
# fill in the data for interval from 1 to 10,interval depends on the data,in some day the interval time is rather bi
#?? function # notice the interval time vary from 1 to .... on different days

weather_filld = (weather_full
    .withColumn('dt_new',F.coalesce(*lag_windows))
    .withColumn('dt', F.when(F.col('dt').isNull(), F.col('dt_new')).otherwise(F.col('dt'))))['ts','dt']
weather_filld.head(100)

full_data= weather_filld.join(weather, on=weather.dt==weather_filld.dt, how='left').drop(weather.dt).drop(weather_filld.dt).drop(weather.id)
full_data.count()

df = weather_filld.filter(weather_filld.dt.isNull()).toPandas()

df.groupby(pd.to_datetime(df.ts).dt.date).count()

ts.filter(ts.ts<'2015-07-01 01:20:55').filter(ts.ts>'2015-07-01 01:20:49').head(100)

trackestimate_subset = track_subset.withColumn('dt', F.date_format('dt', 'yyyy-MM-dd HH:mm:ss'))
trackestimate_subset.count()

data = trackestimate_subset.join(full_data,on=trackestimate_subset.dt==full_data.ts,how='left').drop(full_data.ts).persist()
data.count()

#data.filter(data.relativehumidity.isNull()).count() - data.count() # check if all NULL
#data.filter(data.dewpoint.isNull()).count() - data.count() # check if all NULL
data=data.drop(data.dewpoint).drop(data.relativehumidity)
data=data.withColumn('dt', F.date_format('dt', 'yyyy-MM-dd HH:mm:ss'))
data.count()

data = data.join(track,on=track.id==data.track_id).drop(track.id)
data.head(10)



# the fixed area 
import numpy as np
l_lon = 4.705
r_lat = 52.368
r_lon = 4.815
l_lat = 52.283
dlon =  r_lon-l_lon
dlat =  r_lat-l_lat 
length0 =  r_lon*111.699 * np.cos(r_lat * np.pi/180) - l_lon*111.699 * np.cos(r_lat * np.pi/180)
length = (int(np.ceil(length0*10)))/10.
width0 =  r_lat*110.574 - l_lat*110.574
width =  (int(np.ceil(width0*10)))/10.


# the potential extended distance
#from pyspark.sql.functions import UserDefinedFunction
#from pyspark.sql.types import StringType
#from shapely.wkb import loads
#from shapely import wkt
#track_table = "birds.track"
#track = hc.read.table(track_table) # input track table which contanis trajectory
#track_trajectory = track.select('trajectory')
#def distance(a):
#    return ((a[0][0]*111.321*cos(a[0][1]*np.pi/180) - (a[1][0]*111.321*cos(a[1][1]*np.pi/180)))**2 
#             + ((a[0][1] - a[1][1])*111)**2)**0.5
#udf = UserDefinedFunction(lambda x: str(loads(x,hex=True).__geo_interface__['coordinates']), StringType())
#udf = UserDefinedFunction(lambda x: str(loads(x,hex=True)), StringType())
#a = track_trajectory.withColumn('text', udf(F.col('trajectory')))


# the potential extended distance
track_table = "birds.track"
track = hc.read.table(track_table).where("classification_id != 1")# input track table which contanis trajectory


def max_dist(b):
    return max(((np.array(b)[1:,0]*111.321*np.cos(np.array(b)[1:,1]*np.pi/180) 
                 - b[0][0]*111.321*np.cos(b[0][1]*np.pi/180))**2 + 
                ((np.array(b)[1:,1] - b[0][1])*111)**2)**.5)

udf = UserDefinedFunction(lambda x: str(max_dist(loads(x,hex=True).__geo_interface__['coordinates'])), StringType())
#udf = UserDefinedFunction(lambda x: max_dist(loads(x,hex=True).__geo_interface__['coordinates']), DoubleType())

track_trajectory = (track
                       .select('trajectory')
                       .dropna()
                       .withColumn('max_dist', udf(F.col('trajectory')))
                       .withColumn('max_dist', F.col('max_dist').astype('float'))
                       .select('max_dist')
          )


trajectoy_count = track_trajectory.count()
print trajectoy_count


nth_percentile = (track_trajectory
        .sort(track_trajectory.max_dist.desc())
        .limit(int((1-0.99) * count))
        .sort(track_trajectory.max_dist.asc())
        .first()
)

print nth_percentile


temp.orderBy('cast_trajectoy').first()


lat_det = maxdist/111
log_det = maxdist/(np.cos(l_lat*np.pi/180)*111.321)
bound_x = c(l_lon-log.det,l_lat-lat.det)
bound_y = c(r_lon+log.det,r_lat+lat.det)
n= 10

from shapely.wkb import loads
from pyspark.sql.functions import UserDefinedFunction
from pyspark.sql.types import StringType

udf = UserDefinedFunction(lambda x: str(loads(x,hex=True).__geo_interface__['coordinates'][0])+', '+
                                        str(loads(x,hex=True).__geo_interface__['coordinates'][1]), StringType())

a = track_subset.limit(10).withColumn('position_text', udf(F.col('position')))
